{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariyyah-Alrasheed/Mini_Project_4/blob/main/Ex_Practice_Exercise_on_Convolutional_Neural_Networks_(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a07a343",
      "metadata": {
        "id": "7a07a343"
      },
      "source": [
        "# Practice Exercise on Convolutional Neural Networks (CNN)\n",
        "\n",
        "Welcome to the Practice Exercise on Convolutional Neural Networks (CNN). In this exercise, we will focus on an image classification task where the goal is to predict whether an image contains a cat or a dog. We will work with a dataset of labeled images and build, train, and evaluate a CNN model. This practice will allow you to apply your understanding of CNNs to achieve high accuracy in image classification.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "### **Dataset Name:** Cats and Dogs Image Dataset\n",
        "\n",
        "### **Description:**  \n",
        "The dataset contains images of cats and dogs labeled for classification purposes. Each image belongs to one of the two classes: 'Cat' or 'Dog'. The goal is to classify the images correctly based on the content (i.e., whether the image is of a cat or a dog). The dataset is often used to test image classification models.\n",
        "\n",
        "### **Features:**\n",
        "There are two main folders which are:\n",
        "- `Cat`: Images labeled as containing a cat.\n",
        "- `Dog`: Images labeled as containing a dog.\n",
        "\n",
        "### **Target Variable:**\n",
        "- The goal is to predict whether an image contains a cat or a dog.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb05577",
      "metadata": {
        "id": "3cb05577"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa968f9e",
      "metadata": {
        "id": "fa968f9e"
      },
      "source": [
        "\n",
        "We will start by loading the dataset and preprocessing the images. This includes:\n",
        "- Resizing images .\n",
        "- Normalizing pixel values.\n",
        "\n",
        "Add more if needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "fb0fc5ee",
      "metadata": {
        "id": "fb0fc5ee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3FBfPdVPcC4",
        "outputId": "06412c02-eb8b-40ed-aee8-30da9d16797d"
      },
      "id": "z3FBfPdVPcC4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255,\n",
        "                               rotation_range=20,  # Randomly rotate images in the range (degrees, 0 to 20)\n",
        "    width_shift_range=0.2,  # Randomly translate images horizontally\n",
        "    height_shift_range=0.2,  # Randomly translate images vertically\n",
        "    shear_range=0.2,  # Randomly shear images\n",
        "    zoom_range=0.2,  # Randomly zoom images\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    fill_mode='nearest',  # Fill pixels in images after transformation\n",
        "                             )"
      ],
      "metadata": {
        "id": "GhTYskygP8X5"
      },
      "id": "GhTYskygP8X5",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen_test = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "bxkpXe3oQmz3"
      },
      "id": "bxkpXe3oQmz3",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e2230c4b",
      "metadata": {
        "id": "e2230c4b"
      },
      "source": [
        "## Data Splitting\n",
        "In this section, we will split our dataset into three parts:\n",
        "\n",
        "* Training set (70%): This portion of the dataset is used to train the CNN model.\n",
        "* Validation set (15%): This portion is used to validate the model during training, helping us tune hyperparameters and avoid overfitting.\n",
        "* Test set (15%): This portion is used to evaluate the model after training, to check its generalization to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "AJnZPkQ8sqLD",
      "metadata": {
        "id": "AJnZPkQ8sqLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90091e0a-21de-4b34-a989-7e4d329ccbd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 534 files belonging to 2 classes.\n",
            "Found 534 images belonging to 2 classes.\n",
            "Found 251 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/T5_Mriyyah/Exam/For_DL_exam/archive_4/training_set/training_set')\n",
        "\n",
        "train_set = datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/T5_Mriyyah/Exam/For_DL_exam/archive_4/training_set/training_set',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "\n",
        "val_set = datagen_test.flow_from_directory(\n",
        "    '/content/drive/MyDrive/T5_Mriyyah/Exam/For_DL_exam/archive_4/val_set',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "82f4df4b",
      "metadata": {
        "id": "82f4df4b"
      },
      "outputs": [],
      "source": [
        "label = dataset.class_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TEf30OiRIZk",
        "outputId": "4b969f4d-24e8-4002-94f6-62990c940056"
      },
      "id": "1TEf30OiRIZk",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cats', 'dogs']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_set.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3x2tVW4YWmR",
        "outputId": "8c1a84ef-dd63-4aac-fc11-2d15b2e8f06f"
      },
      "id": "F3x2tVW4YWmR",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cats': 0, 'Dogs': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc3ce960",
      "metadata": {
        "id": "fc3ce960"
      },
      "source": [
        "## Building the CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca1a1cb",
      "metadata": {
        "id": "2ca1a1cb"
      },
      "source": [
        "\n",
        "Now, we will define our CNN architecture using `tensorflow.keras`. The architecture will consist of:\n",
        "- Convolutional layers followed by max-pooling layers\n",
        "- Flatten layer\n",
        "- Dense layers\n",
        "- Output layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "91a5150c",
      "metadata": {
        "id": "91a5150c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8da91c-dc9e-4e35-ea3e-ce08d1a888b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fff4b8",
      "metadata": {
        "id": "e0fff4b8"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84cb05c",
      "metadata": {
        "id": "a84cb05c"
      },
      "source": [
        "\n",
        "Train the CNN model using the `fit` function. We will use the training and validation we created earlier.\n",
        "\n",
        "Fill in the code to train the model for a specified number of epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e11ddcd",
      "metadata": {
        "id": "0e11ddcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aad74f9-a5c1-4b82-8e4f-73b48d7fe599"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 12s/step - accuracy: 0.5084 - loss: 2.0010 - val_accuracy: 0.5458 - val_loss: 0.6931\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 6s/step - accuracy: 0.4883 - loss: 0.6939 - val_accuracy: 0.4542 - val_loss: 0.6938\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 6s/step - accuracy: 0.5116 - loss: 0.6932 - val_accuracy: 0.5458 - val_loss: 0.6930\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step - accuracy: 0.4973 - loss: 0.6932 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.5158 - loss: 0.6930 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step - accuracy: 0.4649 - loss: 0.6936 - val_accuracy: 0.5458 - val_loss: 0.6928\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 7s/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 6s/step - accuracy: 0.4935 - loss: 0.6933 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 6s/step - accuracy: 0.4807 - loss: 0.6934 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 7s/step - accuracy: 0.4836 - loss: 0.6934 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 6s/step - accuracy: 0.5104 - loss: 0.6930 - val_accuracy: 0.5458 - val_loss: 0.6925\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 6s/step - accuracy: 0.5226 - loss: 0.6929 - val_accuracy: 0.5458 - val_loss: 0.6925\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.4957 - loss: 0.6933 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.4982 - loss: 0.6932 - val_accuracy: 0.5458 - val_loss: 0.6925\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - accuracy: 0.5111 - loss: 0.6930 - val_accuracy: 0.5458 - val_loss: 0.6925\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 6s/step - accuracy: 0.4818 - loss: 0.6935 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 7s/step - accuracy: 0.5029 - loss: 0.6931 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 6s/step - accuracy: 0.5067 - loss: 0.6931 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step - accuracy: 0.4804 - loss: 0.6934 - val_accuracy: 0.5458 - val_loss: 0.6927\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 6s/step - accuracy: 0.4726 - loss: 0.6935 - val_accuracy: 0.5458 - val_loss: 0.6927\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 6s/step - accuracy: 0.5096 - loss: 0.6931 - val_accuracy: 0.5458 - val_loss: 0.6927\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 6s/step - accuracy: 0.4947 - loss: 0.6932 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 23/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 6s/step - accuracy: 0.5270 - loss: 0.6928 - val_accuracy: 0.5458 - val_loss: 0.6925\n",
            "Epoch 24/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step - accuracy: 0.4836 - loss: 0.6935 - val_accuracy: 0.5458 - val_loss: 0.6927\n",
            "Epoch 25/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 6s/step - accuracy: 0.5156 - loss: 0.6930 - val_accuracy: 0.5458 - val_loss: 0.6926\n",
            "Epoch 26/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 6s/step - accuracy: 0.4857 - loss: 0.6933 - val_accuracy: 0.5458 - val_loss: 0.6927\n",
            "Epoch 27/50\n",
            "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 6s/step - accuracy: 0.4870 - loss: 0.6933"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_set,\n",
        "          epochs=50,\n",
        "          validation_data=val_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba878f4",
      "metadata": {
        "id": "3ba878f4"
      },
      "source": [
        "## Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d508bce7",
      "metadata": {
        "id": "d508bce7"
      },
      "source": [
        "\n",
        "After training, evaluate the model on the validation data to check its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd3a753",
      "metadata": {
        "id": "5cd3a753"
      },
      "outputs": [],
      "source": [
        "model.evaluate(val_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93e0c1b9",
      "metadata": {
        "id": "93e0c1b9"
      },
      "source": [
        "## Testing with New Images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db94a8ae",
      "metadata": {
        "id": "db94a8ae"
      },
      "source": [
        "Finally, let's test the model with some new images. Preprocess the images and use the trained model to predict whether the image is of a cat or a dog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf379ec",
      "metadata": {
        "id": "1cf379ec"
      },
      "outputs": [],
      "source": [
        "model.predict(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_set)"
      ],
      "metadata": {
        "id": "yFWVDkqgcpPJ"
      },
      "id": "yFWVDkqgcpPJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(test_set.classes, y_pred)"
      ],
      "metadata": {
        "id": "tDihoFhYcs4J"
      },
      "id": "tDihoFhYcs4J",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}